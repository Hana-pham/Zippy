<html>
<head>
<title>_parser.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6a8759;}
.s4 { color: #808080;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_parser.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot;Handwritten parser of dependency specifiers. 
 
The docstring for each __parse_* function contains ENBF-inspired grammar representing 
the implementation. 
&quot;&quot;&quot;</span>

<span class="s2">import </span><span class="s1">ast</span>
<span class="s2">from </span><span class="s1">typing </span><span class="s2">import </span><span class="s1">Any</span><span class="s2">, </span><span class="s1">List</span><span class="s2">, </span><span class="s1">NamedTuple</span><span class="s2">, </span><span class="s1">Optional</span><span class="s2">, </span><span class="s1">Tuple</span><span class="s2">, </span><span class="s1">Union</span>

<span class="s2">from </span><span class="s1">._tokenizer </span><span class="s2">import </span><span class="s1">DEFAULT_RULES</span><span class="s2">, </span><span class="s1">Tokenizer</span>


<span class="s2">class </span><span class="s1">Node:</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">value: str) -&gt; </span><span class="s2">None</span><span class="s1">:</span>
        <span class="s1">self.value = value</span>

    <span class="s2">def </span><span class="s1">__str__(self) -&gt; str:</span>
        <span class="s2">return </span><span class="s1">self.value</span>

    <span class="s2">def </span><span class="s1">__repr__(self) -&gt; str:</span>
        <span class="s2">return </span><span class="s3">f&quot;&lt;</span><span class="s2">{</span><span class="s1">self.__class__.__name__</span><span class="s2">}</span><span class="s3">('</span><span class="s2">{</span><span class="s1">self</span><span class="s2">}</span><span class="s3">')&gt;&quot;</span>

    <span class="s2">def </span><span class="s1">serialize(self) -&gt; str:</span>
        <span class="s2">raise </span><span class="s1">NotImplementedError</span>


<span class="s2">class </span><span class="s1">Variable(Node):</span>
    <span class="s2">def </span><span class="s1">serialize(self) -&gt; str:</span>
        <span class="s2">return </span><span class="s1">str(self)</span>


<span class="s2">class </span><span class="s1">Value(Node):</span>
    <span class="s2">def </span><span class="s1">serialize(self) -&gt; str:</span>
        <span class="s2">return </span><span class="s3">f'&quot;</span><span class="s2">{</span><span class="s1">self</span><span class="s2">}</span><span class="s3">&quot;'</span>


<span class="s2">class </span><span class="s1">Op(Node):</span>
    <span class="s2">def </span><span class="s1">serialize(self) -&gt; str:</span>
        <span class="s2">return </span><span class="s1">str(self)</span>


<span class="s1">MarkerVar = Union[Variable</span><span class="s2">, </span><span class="s1">Value]</span>
<span class="s1">MarkerItem = Tuple[MarkerVar</span><span class="s2">, </span><span class="s1">Op</span><span class="s2">, </span><span class="s1">MarkerVar]</span>
<span class="s4"># MarkerAtom = Union[MarkerItem, List[&quot;MarkerAtom&quot;]]</span>
<span class="s4"># MarkerList = List[Union[&quot;MarkerList&quot;, MarkerAtom, str]]</span>
<span class="s4"># mypy does not support recursive type definition</span>
<span class="s4"># https://github.com/python/mypy/issues/731</span>
<span class="s1">MarkerAtom = Any</span>
<span class="s1">MarkerList = List[Any]</span>


<span class="s2">class </span><span class="s1">ParsedRequirement(NamedTuple):</span>
    <span class="s1">name: str</span>
    <span class="s1">url: str</span>
    <span class="s1">extras: List[str]</span>
    <span class="s1">specifier: str</span>
    <span class="s1">marker: Optional[MarkerList]</span>


<span class="s4"># --------------------------------------------------------------------------------------</span>
<span class="s4"># Recursive descent parser for dependency specifier</span>
<span class="s4"># --------------------------------------------------------------------------------------</span>
<span class="s2">def </span><span class="s1">parse_requirement(source: str) -&gt; ParsedRequirement:</span>
    <span class="s2">return </span><span class="s1">_parse_requirement(Tokenizer(source</span><span class="s2">, </span><span class="s1">rules=DEFAULT_RULES))</span>


<span class="s2">def </span><span class="s1">_parse_requirement(tokenizer: Tokenizer) -&gt; ParsedRequirement:</span>
    <span class="s0">&quot;&quot;&quot; 
    requirement = WS? IDENTIFIER WS? extras WS? requirement_details 
    &quot;&quot;&quot;</span>
    <span class="s1">tokenizer.consume(</span><span class="s3">&quot;WS&quot;</span><span class="s1">)</span>

    <span class="s1">name_token = tokenizer.expect(</span>
        <span class="s3">&quot;IDENTIFIER&quot;</span><span class="s2">, </span><span class="s1">expected=</span><span class="s3">&quot;package name at the start of dependency specifier&quot;</span>
    <span class="s1">)</span>
    <span class="s1">name = name_token.text</span>
    <span class="s1">tokenizer.consume(</span><span class="s3">&quot;WS&quot;</span><span class="s1">)</span>

    <span class="s1">extras = _parse_extras(tokenizer)</span>
    <span class="s1">tokenizer.consume(</span><span class="s3">&quot;WS&quot;</span><span class="s1">)</span>

    <span class="s1">url</span><span class="s2">, </span><span class="s1">specifier</span><span class="s2">, </span><span class="s1">marker = _parse_requirement_details(tokenizer)</span>
    <span class="s1">tokenizer.expect(</span><span class="s3">&quot;END&quot;</span><span class="s2">, </span><span class="s1">expected=</span><span class="s3">&quot;end of dependency specifier&quot;</span><span class="s1">)</span>

    <span class="s2">return </span><span class="s1">ParsedRequirement(name</span><span class="s2">, </span><span class="s1">url</span><span class="s2">, </span><span class="s1">extras</span><span class="s2">, </span><span class="s1">specifier</span><span class="s2">, </span><span class="s1">marker)</span>


<span class="s2">def </span><span class="s1">_parse_requirement_details(</span>
    <span class="s1">tokenizer: Tokenizer</span><span class="s2">,</span>
<span class="s1">) -&gt; Tuple[str</span><span class="s2">, </span><span class="s1">str</span><span class="s2">, </span><span class="s1">Optional[MarkerList]]:</span>
    <span class="s0">&quot;&quot;&quot; 
    requirement_details = AT URL (WS requirement_marker?)? 
                        | specifier WS? (requirement_marker)? 
    &quot;&quot;&quot;</span>

    <span class="s1">specifier = </span><span class="s3">&quot;&quot;</span>
    <span class="s1">url = </span><span class="s3">&quot;&quot;</span>
    <span class="s1">marker = </span><span class="s2">None</span>

    <span class="s2">if </span><span class="s1">tokenizer.check(</span><span class="s3">&quot;AT&quot;</span><span class="s1">):</span>
        <span class="s1">tokenizer.read()</span>
        <span class="s1">tokenizer.consume(</span><span class="s3">&quot;WS&quot;</span><span class="s1">)</span>

        <span class="s1">url_start = tokenizer.position</span>
        <span class="s1">url = tokenizer.expect(</span><span class="s3">&quot;URL&quot;</span><span class="s2">, </span><span class="s1">expected=</span><span class="s3">&quot;URL after @&quot;</span><span class="s1">).text</span>
        <span class="s2">if </span><span class="s1">tokenizer.check(</span><span class="s3">&quot;END&quot;</span><span class="s2">, </span><span class="s1">peek=</span><span class="s2">True</span><span class="s1">):</span>
            <span class="s2">return </span><span class="s1">(url</span><span class="s2">, </span><span class="s1">specifier</span><span class="s2">, </span><span class="s1">marker)</span>

        <span class="s1">tokenizer.expect(</span><span class="s3">&quot;WS&quot;</span><span class="s2">, </span><span class="s1">expected=</span><span class="s3">&quot;whitespace after URL&quot;</span><span class="s1">)</span>

        <span class="s4"># The input might end after whitespace.</span>
        <span class="s2">if </span><span class="s1">tokenizer.check(</span><span class="s3">&quot;END&quot;</span><span class="s2">, </span><span class="s1">peek=</span><span class="s2">True</span><span class="s1">):</span>
            <span class="s2">return </span><span class="s1">(url</span><span class="s2">, </span><span class="s1">specifier</span><span class="s2">, </span><span class="s1">marker)</span>

        <span class="s1">marker = _parse_requirement_marker(</span>
            <span class="s1">tokenizer</span><span class="s2">, </span><span class="s1">span_start=url_start</span><span class="s2">, </span><span class="s1">after=</span><span class="s3">&quot;URL and whitespace&quot;</span>
        <span class="s1">)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">specifier_start = tokenizer.position</span>
        <span class="s1">specifier = _parse_specifier(tokenizer)</span>
        <span class="s1">tokenizer.consume(</span><span class="s3">&quot;WS&quot;</span><span class="s1">)</span>

        <span class="s2">if </span><span class="s1">tokenizer.check(</span><span class="s3">&quot;END&quot;</span><span class="s2">, </span><span class="s1">peek=</span><span class="s2">True</span><span class="s1">):</span>
            <span class="s2">return </span><span class="s1">(url</span><span class="s2">, </span><span class="s1">specifier</span><span class="s2">, </span><span class="s1">marker)</span>

        <span class="s1">marker = _parse_requirement_marker(</span>
            <span class="s1">tokenizer</span><span class="s2">,</span>
            <span class="s1">span_start=specifier_start</span><span class="s2">,</span>
            <span class="s1">after=(</span>
                <span class="s3">&quot;version specifier&quot;</span>
                <span class="s2">if </span><span class="s1">specifier</span>
                <span class="s2">else </span><span class="s3">&quot;name and no valid version specifier&quot;</span>
            <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">)</span>

    <span class="s2">return </span><span class="s1">(url</span><span class="s2">, </span><span class="s1">specifier</span><span class="s2">, </span><span class="s1">marker)</span>


<span class="s2">def </span><span class="s1">_parse_requirement_marker(</span>
    <span class="s1">tokenizer: Tokenizer</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">span_start: int</span><span class="s2">, </span><span class="s1">after: str</span>
<span class="s1">) -&gt; MarkerList:</span>
    <span class="s0">&quot;&quot;&quot; 
    requirement_marker = SEMICOLON marker WS? 
    &quot;&quot;&quot;</span>

    <span class="s2">if not </span><span class="s1">tokenizer.check(</span><span class="s3">&quot;SEMICOLON&quot;</span><span class="s1">):</span>
        <span class="s1">tokenizer.raise_syntax_error(</span>
            <span class="s3">f&quot;Expected end or semicolon (after </span><span class="s2">{</span><span class="s1">after</span><span class="s2">}</span><span class="s3">)&quot;</span><span class="s2">,</span>
            <span class="s1">span_start=span_start</span><span class="s2">,</span>
        <span class="s1">)</span>
    <span class="s1">tokenizer.read()</span>

    <span class="s1">marker = _parse_marker(tokenizer)</span>
    <span class="s1">tokenizer.consume(</span><span class="s3">&quot;WS&quot;</span><span class="s1">)</span>

    <span class="s2">return </span><span class="s1">marker</span>


<span class="s2">def </span><span class="s1">_parse_extras(tokenizer: Tokenizer) -&gt; List[str]:</span>
    <span class="s0">&quot;&quot;&quot; 
    extras = (LEFT_BRACKET wsp* extras_list? wsp* RIGHT_BRACKET)? 
    &quot;&quot;&quot;</span>
    <span class="s2">if not </span><span class="s1">tokenizer.check(</span><span class="s3">&quot;LEFT_BRACKET&quot;</span><span class="s2">, </span><span class="s1">peek=</span><span class="s2">True</span><span class="s1">):</span>
        <span class="s2">return </span><span class="s1">[]</span>

    <span class="s2">with </span><span class="s1">tokenizer.enclosing_tokens(</span>
        <span class="s3">&quot;LEFT_BRACKET&quot;</span><span class="s2">,</span>
        <span class="s3">&quot;RIGHT_BRACKET&quot;</span><span class="s2">,</span>
        <span class="s1">around=</span><span class="s3">&quot;extras&quot;</span><span class="s2">,</span>
    <span class="s1">):</span>
        <span class="s1">tokenizer.consume(</span><span class="s3">&quot;WS&quot;</span><span class="s1">)</span>
        <span class="s1">extras = _parse_extras_list(tokenizer)</span>
        <span class="s1">tokenizer.consume(</span><span class="s3">&quot;WS&quot;</span><span class="s1">)</span>

    <span class="s2">return </span><span class="s1">extras</span>


<span class="s2">def </span><span class="s1">_parse_extras_list(tokenizer: Tokenizer) -&gt; List[str]:</span>
    <span class="s0">&quot;&quot;&quot; 
    extras_list = identifier (wsp* ',' wsp* identifier)* 
    &quot;&quot;&quot;</span>
    <span class="s1">extras: List[str] = []</span>

    <span class="s2">if not </span><span class="s1">tokenizer.check(</span><span class="s3">&quot;IDENTIFIER&quot;</span><span class="s1">):</span>
        <span class="s2">return </span><span class="s1">extras</span>

    <span class="s1">extras.append(tokenizer.read().text)</span>

    <span class="s2">while True</span><span class="s1">:</span>
        <span class="s1">tokenizer.consume(</span><span class="s3">&quot;WS&quot;</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">tokenizer.check(</span><span class="s3">&quot;IDENTIFIER&quot;</span><span class="s2">, </span><span class="s1">peek=</span><span class="s2">True</span><span class="s1">):</span>
            <span class="s1">tokenizer.raise_syntax_error(</span><span class="s3">&quot;Expected comma between extra names&quot;</span><span class="s1">)</span>
        <span class="s2">elif not </span><span class="s1">tokenizer.check(</span><span class="s3">&quot;COMMA&quot;</span><span class="s1">):</span>
            <span class="s2">break</span>

        <span class="s1">tokenizer.read()</span>
        <span class="s1">tokenizer.consume(</span><span class="s3">&quot;WS&quot;</span><span class="s1">)</span>

        <span class="s1">extra_token = tokenizer.expect(</span><span class="s3">&quot;IDENTIFIER&quot;</span><span class="s2">, </span><span class="s1">expected=</span><span class="s3">&quot;extra name after comma&quot;</span><span class="s1">)</span>
        <span class="s1">extras.append(extra_token.text)</span>

    <span class="s2">return </span><span class="s1">extras</span>


<span class="s2">def </span><span class="s1">_parse_specifier(tokenizer: Tokenizer) -&gt; str:</span>
    <span class="s0">&quot;&quot;&quot; 
    specifier = LEFT_PARENTHESIS WS? version_many WS? RIGHT_PARENTHESIS 
              | WS? version_many WS? 
    &quot;&quot;&quot;</span>
    <span class="s2">with </span><span class="s1">tokenizer.enclosing_tokens(</span>
        <span class="s3">&quot;LEFT_PARENTHESIS&quot;</span><span class="s2">,</span>
        <span class="s3">&quot;RIGHT_PARENTHESIS&quot;</span><span class="s2">,</span>
        <span class="s1">around=</span><span class="s3">&quot;version specifier&quot;</span><span class="s2">,</span>
    <span class="s1">):</span>
        <span class="s1">tokenizer.consume(</span><span class="s3">&quot;WS&quot;</span><span class="s1">)</span>
        <span class="s1">parsed_specifiers = _parse_version_many(tokenizer)</span>
        <span class="s1">tokenizer.consume(</span><span class="s3">&quot;WS&quot;</span><span class="s1">)</span>

    <span class="s2">return </span><span class="s1">parsed_specifiers</span>


<span class="s2">def </span><span class="s1">_parse_version_many(tokenizer: Tokenizer) -&gt; str:</span>
    <span class="s0">&quot;&quot;&quot; 
    version_many = (SPECIFIER (WS? COMMA WS? SPECIFIER)*)? 
    &quot;&quot;&quot;</span>
    <span class="s1">parsed_specifiers = </span><span class="s3">&quot;&quot;</span>
    <span class="s2">while </span><span class="s1">tokenizer.check(</span><span class="s3">&quot;SPECIFIER&quot;</span><span class="s1">):</span>
        <span class="s1">span_start = tokenizer.position</span>
        <span class="s1">parsed_specifiers += tokenizer.read().text</span>
        <span class="s2">if </span><span class="s1">tokenizer.check(</span><span class="s3">&quot;VERSION_PREFIX_TRAIL&quot;</span><span class="s2">, </span><span class="s1">peek=</span><span class="s2">True</span><span class="s1">):</span>
            <span class="s1">tokenizer.raise_syntax_error(</span>
                <span class="s3">&quot;.* suffix can only be used with `==` or `!=` operators&quot;</span><span class="s2">,</span>
                <span class="s1">span_start=span_start</span><span class="s2">,</span>
                <span class="s1">span_end=tokenizer.position + </span><span class="s5">1</span><span class="s2">,</span>
            <span class="s1">)</span>
        <span class="s2">if </span><span class="s1">tokenizer.check(</span><span class="s3">&quot;VERSION_LOCAL_LABEL_TRAIL&quot;</span><span class="s2">, </span><span class="s1">peek=</span><span class="s2">True</span><span class="s1">):</span>
            <span class="s1">tokenizer.raise_syntax_error(</span>
                <span class="s3">&quot;Local version label can only be used with `==` or `!=` operators&quot;</span><span class="s2">,</span>
                <span class="s1">span_start=span_start</span><span class="s2">,</span>
                <span class="s1">span_end=tokenizer.position</span><span class="s2">,</span>
            <span class="s1">)</span>
        <span class="s1">tokenizer.consume(</span><span class="s3">&quot;WS&quot;</span><span class="s1">)</span>
        <span class="s2">if not </span><span class="s1">tokenizer.check(</span><span class="s3">&quot;COMMA&quot;</span><span class="s1">):</span>
            <span class="s2">break</span>
        <span class="s1">parsed_specifiers += tokenizer.read().text</span>
        <span class="s1">tokenizer.consume(</span><span class="s3">&quot;WS&quot;</span><span class="s1">)</span>

    <span class="s2">return </span><span class="s1">parsed_specifiers</span>


<span class="s4"># --------------------------------------------------------------------------------------</span>
<span class="s4"># Recursive descent parser for marker expression</span>
<span class="s4"># --------------------------------------------------------------------------------------</span>
<span class="s2">def </span><span class="s1">parse_marker(source: str) -&gt; MarkerList:</span>
    <span class="s2">return </span><span class="s1">_parse_full_marker(Tokenizer(source</span><span class="s2">, </span><span class="s1">rules=DEFAULT_RULES))</span>


<span class="s2">def </span><span class="s1">_parse_full_marker(tokenizer: Tokenizer) -&gt; MarkerList:</span>
    <span class="s1">retval = _parse_marker(tokenizer)</span>
    <span class="s1">tokenizer.expect(</span><span class="s3">&quot;END&quot;</span><span class="s2">, </span><span class="s1">expected=</span><span class="s3">&quot;end of marker expression&quot;</span><span class="s1">)</span>
    <span class="s2">return </span><span class="s1">retval</span>


<span class="s2">def </span><span class="s1">_parse_marker(tokenizer: Tokenizer) -&gt; MarkerList:</span>
    <span class="s0">&quot;&quot;&quot; 
    marker = marker_atom (BOOLOP marker_atom)+ 
    &quot;&quot;&quot;</span>
    <span class="s1">expression = [_parse_marker_atom(tokenizer)]</span>
    <span class="s2">while </span><span class="s1">tokenizer.check(</span><span class="s3">&quot;BOOLOP&quot;</span><span class="s1">):</span>
        <span class="s1">token = tokenizer.read()</span>
        <span class="s1">expr_right = _parse_marker_atom(tokenizer)</span>
        <span class="s1">expression.extend((token.text</span><span class="s2">, </span><span class="s1">expr_right))</span>
    <span class="s2">return </span><span class="s1">expression</span>


<span class="s2">def </span><span class="s1">_parse_marker_atom(tokenizer: Tokenizer) -&gt; MarkerAtom:</span>
    <span class="s0">&quot;&quot;&quot; 
    marker_atom = WS? LEFT_PARENTHESIS WS? marker WS? RIGHT_PARENTHESIS WS? 
                | WS? marker_item WS? 
    &quot;&quot;&quot;</span>

    <span class="s1">tokenizer.consume(</span><span class="s3">&quot;WS&quot;</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">tokenizer.check(</span><span class="s3">&quot;LEFT_PARENTHESIS&quot;</span><span class="s2">, </span><span class="s1">peek=</span><span class="s2">True</span><span class="s1">):</span>
        <span class="s2">with </span><span class="s1">tokenizer.enclosing_tokens(</span>
            <span class="s3">&quot;LEFT_PARENTHESIS&quot;</span><span class="s2">,</span>
            <span class="s3">&quot;RIGHT_PARENTHESIS&quot;</span><span class="s2">,</span>
            <span class="s1">around=</span><span class="s3">&quot;marker expression&quot;</span><span class="s2">,</span>
        <span class="s1">):</span>
            <span class="s1">tokenizer.consume(</span><span class="s3">&quot;WS&quot;</span><span class="s1">)</span>
            <span class="s1">marker: MarkerAtom = _parse_marker(tokenizer)</span>
            <span class="s1">tokenizer.consume(</span><span class="s3">&quot;WS&quot;</span><span class="s1">)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">marker = _parse_marker_item(tokenizer)</span>
    <span class="s1">tokenizer.consume(</span><span class="s3">&quot;WS&quot;</span><span class="s1">)</span>
    <span class="s2">return </span><span class="s1">marker</span>


<span class="s2">def </span><span class="s1">_parse_marker_item(tokenizer: Tokenizer) -&gt; MarkerItem:</span>
    <span class="s0">&quot;&quot;&quot; 
    marker_item = WS? marker_var WS? marker_op WS? marker_var WS? 
    &quot;&quot;&quot;</span>
    <span class="s1">tokenizer.consume(</span><span class="s3">&quot;WS&quot;</span><span class="s1">)</span>
    <span class="s1">marker_var_left = _parse_marker_var(tokenizer)</span>
    <span class="s1">tokenizer.consume(</span><span class="s3">&quot;WS&quot;</span><span class="s1">)</span>
    <span class="s1">marker_op = _parse_marker_op(tokenizer)</span>
    <span class="s1">tokenizer.consume(</span><span class="s3">&quot;WS&quot;</span><span class="s1">)</span>
    <span class="s1">marker_var_right = _parse_marker_var(tokenizer)</span>
    <span class="s1">tokenizer.consume(</span><span class="s3">&quot;WS&quot;</span><span class="s1">)</span>
    <span class="s2">return </span><span class="s1">(marker_var_left</span><span class="s2">, </span><span class="s1">marker_op</span><span class="s2">, </span><span class="s1">marker_var_right)</span>


<span class="s2">def </span><span class="s1">_parse_marker_var(tokenizer: Tokenizer) -&gt; MarkerVar:</span>
    <span class="s0">&quot;&quot;&quot; 
    marker_var = VARIABLE | QUOTED_STRING 
    &quot;&quot;&quot;</span>
    <span class="s2">if </span><span class="s1">tokenizer.check(</span><span class="s3">&quot;VARIABLE&quot;</span><span class="s1">):</span>
        <span class="s2">return </span><span class="s1">process_env_var(tokenizer.read().text.replace(</span><span class="s3">&quot;.&quot;</span><span class="s2">, </span><span class="s3">&quot;_&quot;</span><span class="s1">))</span>
    <span class="s2">elif </span><span class="s1">tokenizer.check(</span><span class="s3">&quot;QUOTED_STRING&quot;</span><span class="s1">):</span>
        <span class="s2">return </span><span class="s1">process_python_str(tokenizer.read().text)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">tokenizer.raise_syntax_error(</span>
            <span class="s1">message=</span><span class="s3">&quot;Expected a marker variable or quoted string&quot;</span>
        <span class="s1">)</span>


<span class="s2">def </span><span class="s1">process_env_var(env_var: str) -&gt; Variable:</span>
    <span class="s2">if </span><span class="s1">env_var </span><span class="s2">in </span><span class="s1">(</span><span class="s3">&quot;platform_python_implementation&quot;</span><span class="s2">, </span><span class="s3">&quot;python_implementation&quot;</span><span class="s1">):</span>
        <span class="s2">return </span><span class="s1">Variable(</span><span class="s3">&quot;platform_python_implementation&quot;</span><span class="s1">)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s2">return </span><span class="s1">Variable(env_var)</span>


<span class="s2">def </span><span class="s1">process_python_str(python_str: str) -&gt; Value:</span>
    <span class="s1">value = ast.literal_eval(python_str)</span>
    <span class="s2">return </span><span class="s1">Value(str(value))</span>


<span class="s2">def </span><span class="s1">_parse_marker_op(tokenizer: Tokenizer) -&gt; Op:</span>
    <span class="s0">&quot;&quot;&quot; 
    marker_op = IN | NOT IN | OP 
    &quot;&quot;&quot;</span>
    <span class="s2">if </span><span class="s1">tokenizer.check(</span><span class="s3">&quot;IN&quot;</span><span class="s1">):</span>
        <span class="s1">tokenizer.read()</span>
        <span class="s2">return </span><span class="s1">Op(</span><span class="s3">&quot;in&quot;</span><span class="s1">)</span>
    <span class="s2">elif </span><span class="s1">tokenizer.check(</span><span class="s3">&quot;NOT&quot;</span><span class="s1">):</span>
        <span class="s1">tokenizer.read()</span>
        <span class="s1">tokenizer.expect(</span><span class="s3">&quot;WS&quot;</span><span class="s2">, </span><span class="s1">expected=</span><span class="s3">&quot;whitespace after 'not'&quot;</span><span class="s1">)</span>
        <span class="s1">tokenizer.expect(</span><span class="s3">&quot;IN&quot;</span><span class="s2">, </span><span class="s1">expected=</span><span class="s3">&quot;'in' after 'not'&quot;</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">Op(</span><span class="s3">&quot;not in&quot;</span><span class="s1">)</span>
    <span class="s2">elif </span><span class="s1">tokenizer.check(</span><span class="s3">&quot;OP&quot;</span><span class="s1">):</span>
        <span class="s2">return </span><span class="s1">Op(tokenizer.read().text)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s2">return </span><span class="s1">tokenizer.raise_syntax_error(</span>
            <span class="s3">&quot;Expected marker operator, one of &quot;</span>
            <span class="s3">&quot;&lt;=, &lt;, !=, ==, &gt;=, &gt;, ~=, ===, in, not in&quot;</span>
        <span class="s1">)</span>
</pre>
</body>
</html>